# BLUB Ocean — Полный Code Review + Рефакторинг

## Контекст для Claude Code

Ты — senior ML engineer, проводящий полный аудит кодовой базы BLUB Ocean. Это проект где 33 AI-агента (лобстеры) должны эволюционировать язык из 10 бессмысленных звуков через экономическое давление. **Язык не возникает.** После 1000+ эпох и множества инкрементальных фиксов, ключевые метрики мертвы:

```
CIC    = 0.01   (target >0.05)  — сообщения НЕ влияют на поведение
Vocab  = 0      (target >30)    — язык коллапсирует к одному сообщению
MI     = 0.91   (было 4.08)     — непрерывно падает, информация теряется
TopSim = 0.05   (target >0.20)  — нет композиционности
PosDis = 0.20   (target >0.30)  — нестабильно, осциллирует
CSR    = 0.37   (target >0.50)  — слабая корреляция контекст-звук
```

### Что уже пробовали (не помогло):
1. Фикс pos1 context: `(sit, urg, 0, tgt, 0, 0)` — PosDis чуть вырос, потом упал
2. Causal influence reward (coeff=3.0) — influence_score = 0.00 ВСЕГДА (серверный баг)
3. Entropy regularization (cap ±0.3) — при vocab=0 это постоянный penalty -0.3
4. Population heterogeneity (lr × uniform 0.7-1.4) — без видимого эффекта
5. 1/σ² и (n-1) в формуле градиента — уже на месте, не помогает

### Что увидели в debug:
- `advantage` варьируется (-28 до +34) — это нормально
- `credit_part` доминирует когда ненулевой — это нормально  
- `influence = 0.00` в 100% случаев — баг на сервере
- `entropy_adj = -0.30` всегда (потому что vocab=0 → H=0)
- `sigma = 1.0` при n_sounds=10
- `mu ≈ 0.5` для всех агентов → center ≈ 5 → все говорят звук 4-6

---

## Задание: Полный аудит в 4 шага

### ШАГ 1: Прочитай и пойми ВСЮ систему (НЕ МЕНЯЙ НИЧЕГО)

Прочитай эти файлы полностью, от первой до последней строки:

1. `agents/social_agent.py` — ВЕСЬ файл. Особенно:
   - `class GaussianProductionPolicy` — как именно produce() генерирует звук
   - `reinforce()` — полная формула обновления: что входит в advantage, как считается gradient, как применяется к весам W
   - `split_context()` — как context делится между позициями
   - `_factored_to_ctx_key()` — маппинг факторов на позиции
   - `anneal_sigma()` — как sigma меняется со временем
   - `think()` — где вызывается reinforce, откуда приходят rewards
   - `_update_beliefs()` — Bayesian listener
   - Cultural cache: `deposit_to_cultural_cache()` и `inherit_from_cultural_cache()`
   - `reset_language()` — что происходит при перерождении агента

2. `server/ocean.py` — ВЕСЬ файл. Особенно:
   - Tick loop: что происходит каждый тик в каком порядке
   - Sound propagation: как сообщения доставляются слушателям
   - Rift reward calculation: как считаются кредиты
   - `_heard_from` tracking: что записывается
   - `_speaker_influence` tracking: как считается influence_score
   - `_resolve_speaker_influence()` или `_resolve_cic_events()`: логика определения "moved toward rift"
   - `get_agent_state()`: что отправляется агенту
   - Epoch reset: что обнуляется, что сохраняется
   - Predator logic: как хищники влияют на агентов
   - Colony logic: DBSCAN, бонусы

3. `config.yaml` — все параметры, их значения

### ШАГ 2: Диагностика — найди ВСЕ проблемы

После прочтения кода, ответь на каждый вопрос. Не угадывай — цитируй конкретные строки.

#### A. Gradient Flow (почему mu не учится)

A1. Покажи полный путь от "агент заработал кредиты у рифта" до "W[pos] обновился":
- Где кредиты считаются? (файл, строка)
- Как передаются агенту? (через какое поле в state)
- Как агент их читает? (файл, строка)
- Как превращаются в reward для reinforce? (файл, строка)
- Как reward превращается в advantage? (формула, строка)
- Как advantage превращается в grad_mu? (формула, строка)
- Как grad_mu применяется к весам? (формула, строка)

A2. Есть ли разрыв в этой цепочке? Может ли случиться что агент заработал кредиты, но reward = 0?

A3. Как часто агент вообще говорит? (условие для speak в think()). Сколько reinforce() вызовов за эпоху в среднем?

A4. `self.baseline[pos]` — как обновляется? Если baseline = среднему reward, то advantage ≈ 0 большую часть времени. Формула baseline, скорость обновления.

A5. sigma = 1.0 при n_sounds = 10. Покажи anneal schedule. Сколько эпох нужно чтобы sigma дошла до 0.5? До 0.3? Не слишком ли медленно?

A6. Cultural cache при смерти: какой % весов W сбрасывается? Если агент за 200 тиков сдвинул mu на 0.1, а при перерождении теряет 60% — net progress = 0.04 за жизнь. Этого достаточно?

A7. Есть ли weight decay, gradient clipping, momentum, или другие регуляризации которые замедляют обучение?

#### B. Influence Reward Bug (почему influence = 0 всегда)

B1. Покажи точный код `_resolve_speaker_influence()` (или как он называется).
B2. Вызывается ли этот метод вообще? В каком месте tick loop?
B3. `_cic_speaker_pending` — как заполняется? Покажи код.
B4. Проверка distance: как вычисляется? `distance_to_nearest_rift()` — есть ли такой метод?
B5. `get_agent_state()` — покажи строку где influence_score включается в state dict.
B6. Может ли быть что pending очищается ДО резолва?

#### C. Vocab Collapse (почему все говорят одинаково)

C1. `produce()` — покажи полный код. Как mu превращается в center, как center превращается в sound index.
C2. Если mu=0.50 → center=4.5 → sample ∈ N(4.5, 1.0) → clip to [0,9] → round. Какова вероятность получить звук 4? Звук 5? Звук 0? Звук 9?
C3. Разные контексты дают разный mu? Покажи forward pass: ctx → mu. Если W инициализирован нулями, что дают разные контексты?
C4. Есть ли exploration помимо sigma? Epsilon-greedy? Temperature? Что мешает коллапсу?

#### D. MI Decay (почему информация теряется каждое поколение)

D1. Bottleneck: каждые 200 тиков — кто умирает? Lowest earner. Что наследует новорождённый? Покажи код.
D2. Наследование 40% от cultural cache — это 40% от ЧЬИХ весов? Последнего умершего? Среднего? Лучшего?
D3. Слушатель: `_update_beliefs()` — как быстро Bayesian counts накапливаются? При 33 агентах, сколько observations за 200 тиков?
D4. Информация в Bayesian counts vs информация в W (policy weights). Что наследуется, что теряется?

#### E. Architectural Issues

E1. Порядок операций в tick: move → speak → listen → reward → learn? Или другой? Есть ли race conditions?
E2. Сообщение стоит 5 кредитов. Solo rift = 0.5/тик. Нужно 10 тиков молчания чтобы заговорить один раз. Это слишком дорого?
E3. Listener feedback: speaker получает 30% от listener reward. Как это реализовано? Может ли быть что speaker не получает feedback вовремя для reinforce?
E4. Есть ли timing issue: агент говорит → reward приходит через N тиков → reinforce() связывает с ТЕКУЩИМ контекстом, а не с контекстом когда говорил?

### ШАГ 3: Рейтинг проблем

После диагностики — составь таблицу ВСЕХ найденных проблем:

```
| # | Проблема | Файл:строка | Severity | Описание | Фикс |
```

Severity:
- **CRITICAL** — без этого фикса язык не может возникнуть в принципе
- **HIGH** — сильно замедляет обучение
- **MEDIUM** — ухудшает качество но не блокирует
- **LOW** — nice to have

Отсортируй по severity, потом по простоте фикса.

### ШАГ 4: План рефакторинга

Для каждой CRITICAL и HIGH проблемы:
1. Точное описание фикса (что менять, на что)
2. Ожидаемый эффект на метрики
3. Как проверить что фикс работает

Фиксы должны быть:
- **Минимальными** — не переписывать всё, а точечно исправлять
- **Тестируемыми** — после каждого фикса можно прогнать 50 эпох и увидеть эффект
- **Последовательными** — порядок деплоя, по одному

---

## Пять священных принципов (НЕ НАРУШАТЬ)

1. **Никогда не хардкодить роли, конвенции или язык** — всё должно EMERGE
2. **Никаких нейросетей** — простые агенты под давлением
3. **Не одно доминирующее давление** — food + predator + social + economic
4. **Никогда не делать информацию глобально наблюдаемой** — локальность обязательна
5. **Никогда не оптимизировать одну метрику** — портфель метрик

---

## Формат ответа

Сначала ШАГ 1 (прочитай всё молча).
Потом ШАГ 2 (ответы на каждый вопрос A1-E4 с цитатами кода).
Потом ШАГ 3 (таблица проблем).
Потом ШАГ 4 (план фиксов).

**НЕ НАЧИНАЙ МЕНЯТЬ КОД** пока не пройдёшь все 4 шага и не получишь от меня подтверждение. Я хочу сначала увидеть диагностику.
